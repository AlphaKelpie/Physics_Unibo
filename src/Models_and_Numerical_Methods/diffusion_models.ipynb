{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion models\n",
    "\n",
    "All of this is basically an application of compression theory.\n",
    "\n",
    "Let's [read a new paper](https://arxiv.org/pdf/2209.00796.pdf) and highlight important things.\n",
    "\n",
    "Diffusion models are deep generative models, used for images, videos, and molecular design.\n",
    "[First fundamental paper](https://arxiv.org/pdf/1503.03585.pdf) is from 2015, so very recent.\n",
    "[Another one, on denoising diffusion models](https://arxiv.org/pdf/2006.11239.pdf) from 2019.\n",
    "This field is moving very fast.\n",
    "Main application are text-to-image one, but there are many fields.\n",
    "\n",
    "Up to now, we saw encoders, GANs, and so on.\n",
    "Also, score-based models use stochastic differential equations.\n",
    "\n",
    "How to?\n",
    "We take sample from real data, then add infinitesimal noise to it (gaussian perturbation).\n",
    "Then we basically destroy all information: a forward process (Markov gaussian process) will go from image to full noise.\n",
    "Next, we add a neural network to learn the reverse: we **learn an impossible process**!\n",
    "We're actually asking a suitable neural network to remove noise from an image, i.e. to create information.\n",
    "It will still be a combination of gaussian processes, but a complex one.\n",
    "\n",
    "Let's go into the math.\n",
    "From the first paper we see measures, factorization, entropy... all topics of this course.\n",
    "We've two Markov chains: one forward and one reverse.\n",
    "The forward one adds noise, the reverse removes it by learning transition kernels.\n",
    "New data points are generated from random vectors (completely gaussian images) going through the reverse chain.\n",
    "As physicist, we can think this process with generic data (e.g. non-euclidean).\n",
    "\n",
    "Let $\\vec{x}_0$ be a point in our space, i.e. an image, following a distribution $q(\\vec{x}_0)$.\n",
    "Then, with the forward Markov chain we generate a sequence of random variables using transition kernels $q(\\vec{x}_t|\\vec{x}_{t-1})$.\n",
    "As we now, we can factorize the process due to the chain property.\n",
    "What's a transition kernel?\n",
    "It could be any metrics which satisfies a normalization condition.\n",
    "In particular, we choose a gaussian distribution which has a mean equal to the one of the previous one shifted by a little (perturbed).\n",
    "In the same way we change also the variance.\n",
    "This shift $\\beta$ is a hyperparameter of the model.\n",
    "Each pixel of the next image is generated by the gaussian, **independently**.\n",
    "Why gaussian?\n",
    "Well, the composition of more gaussian is still a gaussian... which is a very useful property.\n",
    "\n",
    "Given that\n",
    "\n",
    "$\\vec{x}_t = \\sqrt{\\alpha_t}\\vec{x}_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon_1$\n",
    "\n",
    "$\\vec{x}_{t-1} = \\sqrt{\\alpha_{t-1}}\\vec{x}_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon_2$\n",
    "\n",
    "if $\\vec{x}_3 = \\vec{x}_2 + \\vec{x}_1$ then it follows $\\mathcal{N}\\left(\\bar{x}_1 + \\bar{x}_2, \\sigma^2_1 + \\sigma^2_2\\right)$\n",
    "\n",
    "So, we can compose the entire process.\n",
    "Given $\\vec{x}_0$ we add the first noise $\\epsilon$ normally distributed.\n",
    "We're not destroying the image, we're destroying its distribution (and also the image itself).\n",
    "Once destroyed all the things, we try to generate from noise trying to remove it and obtain a clear image.\n",
    "We actually want to remove noise and keep a structure.\n",
    "We try to go back in time, i.e. using $p_\\theta(\\vec{x}_{t-1}|\\vec{x}_t)$ reverse process.\n",
    "This process is still a gaussian, but a non-trivial one.\n",
    "Average and standard deviation have a complex form (matrices), but the idea is the same.\n",
    "Now we can sample and training, minimizing the Kullback-Leiber divergence.\n",
    "Due to measure factorization, we can write the KL divergence as a sum.\n",
    "The KL of gaussian distributions is going to be a $\\mathbb{L}^2$ norm of the means.\n",
    "\n",
    "That's it.\n",
    "We're free to choose the neural network to use.\n",
    "One can find more articles [here](https://github.com/heejkoo/Awesome-Diffusion-Models). "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
